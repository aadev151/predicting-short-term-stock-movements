{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# XGBoost – Next-Day Direction Classifier\n",
    "\n",
    "Gradient-boosted trees typically outperform single trees and logistic regression on tabular financial data.  \n",
    "Key advantages for this dataset:\n",
    "- **Handles missing values natively** — no imputation required for sparse macro / sentiment columns.\n",
    "- **Non-linear feature interactions** captured automatically.\n",
    "- **SHAP values** provide per-prediction explanations.\n",
    "\n",
    "**Time-based split:** train `< 2023-01-01`, test `≥ 2023-01-01`\n",
    "\n",
    "> Install requirements if needed:  \n",
    "> `pip install xgboost shap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print('SHAP available – full feature explanation will run.')\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print('SHAP not installed – skipping SHAP section. Run: pip install shap')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-load",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/merged_dataset.csv', parse_dates=['date'])\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Date range: {df[\"date\"].min().date()} – {df[\"date\"].max().date()}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-feat",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "XGBoost handles `NaN` natively, so sparse macro and sentiment columns are **not imputed** here — they're passed as-is, letting XGBoost learn the missing-value splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-feat",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "# ── Target: next-day direction ────────────────────────────────────────────────\n",
    "df['next_return'] = df.groupby('ticker')['daily_return'].shift(-1)\n",
    "df['target'] = (df['next_return'] > 0).astype(int)\n",
    "\n",
    "# ── Lagged returns ────────────────────────────────────────────────────────────\n",
    "df['lag_return_1'] = df['daily_return']\n",
    "df['lag_return_2'] = df.groupby('ticker')['daily_return'].shift(1)\n",
    "df['lag_return_5'] = df.groupby('ticker')['daily_return'].shift(4)\n",
    "\n",
    "# ── Cumulative multi-day returns ──────────────────────────────────────────────\n",
    "df['cum_return_5']  = df.groupby('ticker')['adj_close'].transform(\n",
    "    lambda x: x.pct_change(5)\n",
    ")\n",
    "df['cum_return_10'] = df.groupby('ticker')['adj_close'].transform(\n",
    "    lambda x: x.pct_change(10)\n",
    ")\n",
    "\n",
    "# ── Price vs 20-day MA ────────────────────────────────────────────────────────\n",
    "df['price_to_ma20'] = (\n",
    "    df['close'] / df['rolling_mean_20'].replace(0, np.nan)\n",
    ").replace([np.inf, -np.inf], np.nan) - 1\n",
    "\n",
    "# ── Intraday range ────────────────────────────────────────────────────────────\n",
    "df['hl_range'] = (df['high'] - df['low']) / df['close'].replace(0, np.nan)\n",
    "\n",
    "# ── Overnight gap ─────────────────────────────────────────────────────────────\n",
    "df['prev_close'] = df.groupby('ticker')['close'].shift(1)\n",
    "df['oc_gap'] = (\n",
    "    (df['open'] - df['prev_close']) / df['prev_close'].replace(0, np.nan)\n",
    ").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# ── Abnormal volume ───────────────────────────────────────────────────────────\n",
    "df['vol_20ma'] = df.groupby('ticker')['volume'].transform(\n",
    "    lambda x: x.rolling(20, min_periods=1).mean()\n",
    ")\n",
    "df['vol_norm'] = (df['volume'] / df['vol_20ma'].replace(0, np.nan)).clip(0, 10)\n",
    "\n",
    "# ── Realised volatility (rolling 20d std of log-return) ──────────────────────\n",
    "df['rv_20'] = df.groupby('ticker')['log_return'].transform(\n",
    "    lambda x: x.rolling(20, min_periods=5).std() * np.sqrt(252)\n",
    ")\n",
    "\n",
    "# ── News flag + log news count ────────────────────────────────────────────────\n",
    "df['has_news']       = (df['news_count'].fillna(0) > 0).astype(float)\n",
    "df['log_news_count'] = np.log1p(df['news_count'].fillna(0))\n",
    "\n",
    "# ── Log market cap ────────────────────────────────────────────────────────────\n",
    "df['log_marketcap'] = np.log1p(df['Marketcap'].fillna(0))\n",
    "\n",
    "# ── Sector encoding ───────────────────────────────────────────────────────────\n",
    "df['Sector_encoded'] = df['Sector'].astype('category').cat.codes\n",
    "\n",
    "# NOTE: VIX, Yield_Spread, Regime_GMM, sentiment columns are left as-is (NaN);\n",
    "# XGBoost handles missing values natively via learned split directions.\n",
    "\n",
    "print(f'Shape after feature engineering: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-select",
   "metadata": {},
   "source": [
    "## 3. Feature Selection & Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-select",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS = [\n",
    "    # Price / return signals\n",
    "    'lag_return_1', 'lag_return_2', 'lag_return_5',\n",
    "    'cum_return_5', 'cum_return_10',\n",
    "    'rolling_std_20', 'rv_20',\n",
    "    'price_to_ma20',\n",
    "    # Intraday / volume\n",
    "    'hl_range', 'oc_gap', 'vol_norm',\n",
    "    # Macro (sparse – NaN left as-is)\n",
    "    'VIX', 'Yield_Spread', 'Regime_GMM',\n",
    "    # Sentiment (sparse – NaN left as-is)\n",
    "    'sentiment_mean', 'sentiment_ratio', 'has_news', 'log_news_count',\n",
    "    # Fundamentals\n",
    "    'log_marketcap', 'Revenuegrowth', 'Weight',\n",
    "    # Categorical\n",
    "    'Sector_encoded',\n",
    "]\n",
    "\n",
    "# Only drop rows where the target itself is NaN (last day per ticker)\n",
    "model_df = df[FEATURE_COLS + ['target', 'date']].dropna(subset=['target'])\n",
    "# Also require core price features\n",
    "model_df = model_df.dropna(subset=['lag_return_1', 'lag_return_2'])\n",
    "\n",
    "print(f'Rows: {len(model_df):,}')\n",
    "print(f'Class balance — Up: {model_df[\"target\"].mean():.2%}')\n",
    "\n",
    "SPLIT_DATE = '2023-01-01'\n",
    "train = model_df[model_df['date'] <  SPLIT_DATE]\n",
    "test  = model_df[model_df['date'] >= SPLIT_DATE]\n",
    "\n",
    "X_train, y_train = train[FEATURE_COLS], train['target'].astype(int)\n",
    "X_test,  y_test  = test[FEATURE_COLS],  test['target'].astype(int)\n",
    "\n",
    "naive = max(y_test.mean(), 1 - y_test.mean())\n",
    "print(f'Train: {len(train):,}  |  Test: {len(test):,}')\n",
    "print(f'Naive baseline accuracy: {naive:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-train",
   "metadata": {},
   "source": [
    "## 4. Train XGBoost\n",
    "\n",
    "`eval_metric='logloss'` with early stopping prevents overfitting; a validation slice (last 10% of train by time) monitors performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last 10% of training period as validation for early stopping\n",
    "val_cutoff = train['date'].quantile(0.9)\n",
    "tr   = train[train['date'] <  val_cutoff]\n",
    "val  = train[train['date'] >= val_cutoff]\n",
    "\n",
    "X_tr,  y_tr  = tr[FEATURE_COLS],  tr['target'].astype(int)\n",
    "X_val, y_val = val[FEATURE_COLS], val['target'].astype(int)\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators      = 1000,      # upper bound; early stopping will cut this\n",
    "    learning_rate     = 0.05,\n",
    "    max_depth         = 6,\n",
    "    subsample         = 0.8,\n",
    "    colsample_bytree  = 0.8,\n",
    "    min_child_weight  = 50,        # regularise leaf size for 1M+ rows\n",
    "    reg_alpha         = 0.1,       # L1\n",
    "    reg_lambda        = 1.0,       # L2\n",
    "    scale_pos_weight  = 1,\n",
    "    eval_metric       = 'logloss',\n",
    "    early_stopping_rounds = 30,\n",
    "    random_state      = 42,\n",
    "    n_jobs            = -1,\n",
    "    verbosity         = 0,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=100,\n",
    ")\n",
    "print(f'\\nBest iteration: {model.best_iteration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-eval",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f'Accuracy : {accuracy_score(y_test, y_pred):.4f}  (naive baseline: {naive:.4f})')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, target_names=['Down', 'Up']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-fi",
   "metadata": {},
   "source": [
    "## 6. Built-in Feature Importance\n",
    "\n",
    "Three XGBoost importance types:\n",
    "- **weight** – number of times a feature is used in a split\n",
    "- **gain** – average information gain per split (most informative)\n",
    "- **cover** – average number of samples per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-fi",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_types = ['weight', 'gain', 'cover']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "for ax, imp_type in zip(axes, importance_types):\n",
    "    scores = model.get_booster().get_score(importance_type=imp_type)\n",
    "    fi_df = (\n",
    "        pd.DataFrame.from_dict(scores, orient='index', columns=['importance'])\n",
    "        .sort_values('importance', ascending=True)\n",
    "    )\n",
    "    ax.barh(fi_df.index, fi_df['importance'], color='steelblue')\n",
    "    ax.set_title(f'Feature Importance ({imp_type})')\n",
    "    ax.set_xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-shap",
   "metadata": {},
   "source": [
    "## 7. SHAP Values – Explainability\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) gives each feature a per-prediction contribution.  \n",
    "The **beeswarm plot** shows both the magnitude and direction of impact across all test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-shap",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Sample for speed (SHAP on full test set can be slow)\n",
    "    shap_sample = X_test.sample(min(10_000, len(X_test)), random_state=42)\n",
    "    \n",
    "    explainer    = shap.TreeExplainer(model)\n",
    "    shap_values  = explainer.shap_values(shap_sample)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, shap_sample, show=False)\n",
    "    plt.title('SHAP Beeswarm – Feature Impact on \"Up\" Prediction')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Bar plot of mean |SHAP|\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, shap_sample, plot_type='bar', show=False)\n",
    "    plt.title('Mean |SHAP| – Global Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('SHAP not available. Install with: pip install shap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-sector",
   "metadata": {},
   "source": [
    "## 8. Accuracy by Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "results = test[['date', 'Sector_encoded']].copy()\n",
    "results['y_true'] = y_test.values\n",
    "results['correct'] = (results['y_true'] == y_pred).astype(int)\n",
    "\n",
    "# Decode sector\n",
    "sector_cat = df['Sector'].astype('category').cat\n",
    "code_to_sector = dict(enumerate(sector_cat.categories))\n",
    "results['Sector'] = results['Sector_encoded'].map(code_to_sector)\n",
    "\n",
    "sector_acc = results.groupby('Sector')['correct'].mean().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "colors = ['seagreen' if v >= naive else 'salmon' for v in sector_acc]\n",
    "sector_acc.plot.bar(color=colors)\n",
    "plt.axhline(naive, color='black', ls='--', lw=0.8, label=f'Naive baseline ({naive:.3f})')\n",
    "plt.axhline(acc,   color='blue',  ls='--', lw=0.8, label=f'Overall accuracy ({acc:.3f})')\n",
    "plt.title('XGBoost Accuracy by Sector (Test Set)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
