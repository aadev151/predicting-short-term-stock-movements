{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# Random Forest – Next-Day Direction Classifier\n",
    "\n",
    "Binary classification: predict whether a stock's **next-day return is positive (1) or negative/zero (0)**.\n",
    "\n",
    "An ensemble of 300 trees reduces variance significantly over a single decision tree and can handle non-linear feature interactions.\n",
    "\n",
    "**Time-based split:** train `< 2023-01-01`, test `≥ 2023-01-01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-load",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/merged_dataset.csv', parse_dates=['date'])\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Date range: {df[\"date\"].min().date()} – {df[\"date\"].max().date()}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-feat",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-feat",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n",
    "\n",
    "# ── Target: next-day direction ────────────────────────────────────────────────\n",
    "df['next_return'] = df.groupby('ticker')['daily_return'].shift(-1)\n",
    "df['target'] = (df['next_return'] > 0).astype(int)\n",
    "\n",
    "# ── Lagged returns ────────────────────────────────────────────────────────────\n",
    "df['lag_return_1'] = df['daily_return']\n",
    "df['lag_return_2'] = df.groupby('ticker')['daily_return'].shift(1)\n",
    "df['lag_return_5'] = df.groupby('ticker')['daily_return'].shift(4)\n",
    "\n",
    "# ── Price vs 20-day MA ────────────────────────────────────────────────────────\n",
    "df['price_to_ma20'] = (\n",
    "    df['close'] / df['rolling_mean_20'].replace(0, np.nan)\n",
    ").replace([np.inf, -np.inf], np.nan) - 1\n",
    "\n",
    "# ── Intraday range ────────────────────────────────────────────────────────────\n",
    "df['hl_range'] = (df['high'] - df['low']) / df['close'].replace(0, np.nan)\n",
    "\n",
    "# ── Overnight gap ─────────────────────────────────────────────────────────────\n",
    "df['prev_close'] = df.groupby('ticker')['close'].shift(1)\n",
    "df['oc_gap'] = (\n",
    "    (df['open'] - df['prev_close']) / df['prev_close'].replace(0, np.nan)\n",
    ").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# ── Abnormal volume ───────────────────────────────────────────────────────────\n",
    "df['vol_20ma'] = df.groupby('ticker')['volume'].transform(\n",
    "    lambda x: x.rolling(20, min_periods=1).mean()\n",
    ")\n",
    "df['vol_norm'] = (df['volume'] / df['vol_20ma'].replace(0, np.nan)).clip(0, 10)\n",
    "\n",
    "# ── News flag ─────────────────────────────────────────────────────────────────\n",
    "df['has_news'] = (df['news_count'].fillna(0) > 0).astype(float)\n",
    "\n",
    "# ── Log market cap ────────────────────────────────────────────────────────────\n",
    "df['log_marketcap'] = np.log1p(df['Marketcap'].fillna(0))\n",
    "\n",
    "# ── Impute sparse columns ─────────────────────────────────────────────────────\n",
    "df['VIX']             = df['VIX'].fillna(df['VIX'].median())\n",
    "df['Yield_Spread']    = df['Yield_Spread'].fillna(df['Yield_Spread'].median())\n",
    "df['Regime_GMM']      = df['Regime_GMM'].fillna(df['Regime_GMM'].median())\n",
    "df['sentiment_mean']  = df['sentiment_mean'].fillna(0)\n",
    "df['sentiment_ratio'] = df['sentiment_ratio'].fillna(0)\n",
    "df['Revenuegrowth']   = df['Revenuegrowth'].fillna(0)\n",
    "df['Weight']          = df['Weight'].fillna(0)\n",
    "\n",
    "# ── Sector encoding ───────────────────────────────────────────────────────────\n",
    "df['Sector_encoded'] = df['Sector'].astype('category').cat.codes\n",
    "\n",
    "print(f'Shape after feature engineering: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-select",
   "metadata": {},
   "source": [
    "## 3. Feature Selection & Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-select",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLS = [\n",
    "    'lag_return_1', 'lag_return_2', 'lag_return_5',\n",
    "    'rolling_std_20', 'price_to_ma20',\n",
    "    'hl_range', 'oc_gap', 'vol_norm',\n",
    "    'VIX', 'Yield_Spread', 'Regime_GMM',\n",
    "    'sentiment_mean', 'sentiment_ratio', 'has_news',\n",
    "    'log_marketcap', 'Revenuegrowth', 'Weight',\n",
    "    'Sector_encoded',\n",
    "]\n",
    "\n",
    "model_df = df[FEATURE_COLS + ['target', 'date']].dropna()\n",
    "print(f'Rows after dropna: {len(model_df):,}')\n",
    "\n",
    "SPLIT_DATE = '2023-01-01'\n",
    "train = model_df[model_df['date'] <  SPLIT_DATE]\n",
    "test  = model_df[model_df['date'] >= SPLIT_DATE]\n",
    "\n",
    "X_train, y_train = train[FEATURE_COLS].values, train['target'].values\n",
    "X_test,  y_test  = test[FEATURE_COLS].values,  test['target'].values\n",
    "\n",
    "naive = max(y_test.mean(), 1 - y_test.mean())\n",
    "print(f'Train: {len(train):,}  |  Test: {len(test):,}')\n",
    "print(f'Naive baseline accuracy: {naive:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-rf",
   "metadata": {},
   "source": [
    "## 4. Train Random Forest\n",
    "\n",
    "An ensemble of 300 trees reduces variance significantly over a single tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-rf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=8,\n",
    "    min_samples_leaf=200,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print('=== Random Forest ===')\n",
    "print(f'Accuracy : {accuracy_score(y_test, y_pred_rf):.4f}  (naive: {naive:.4f})')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Down', 'Up']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-importance",
   "metadata": {},
   "source": [
    "## 5. Feature Importance\n",
    "\n",
    "Mean decrease in impurity across all 300 trees gives a reliable importance ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df = pd.DataFrame({\n",
    "    'feature':    FEATURE_COLS,\n",
    "    'importance': rf.feature_importances_,\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.barh(fi_df['feature'], fi_df['importance'], color='seagreen')\n",
    "ax.set_title('Random Forest Feature Importance')\n",
    "ax.set_xlabel('Importance (mean decrease in impurity)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
